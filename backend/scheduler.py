from apscheduler.schedulers.background import BackgroundScheduler
from datetime import datetime
from zoneinfo import ZoneInfo
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging

# Services
from services import stock_service, macro_service, bond_service, analysis_service

# Configure Logging
class PyKrxFilter(logging.Filter):
    def filter(self, record):
        # pykrx ë¼ì´ë¸ŒëŸ¬ë¦¬ ë‚´ë¶€ì—ì„œ ë°œìƒí•˜ëŠ” ë¡œê·¸ëŠ” ë¬´ì¡°ê±´ ì°¨ë‹¨ (í¬ë§·íŒ… ë²„ê·¸ ë°©ì§€)
        # record.pathnameì´ 'pykrx'ë¥¼ í¬í•¨í•˜ë©´ False ë°˜í™˜
        if record.pathname and "pykrx" in record.pathname:
            return False
        return True

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Root Loggerì— í•„í„° ì¶”ê°€ (pykrxê°€ root loggerë¥¼ ì“°ë”ë¼ë„ ì°¨ë‹¨ë¨)
logging.getLogger().addFilter(PyKrxFilter())

# Global Data Store (In-Memory)
DATA_STORE = {
    "market_pulse": [],
    "cpi": {},
    "unrate": {},
    "risk_ratio": [],
    "credit_spread": [],
    "yield_gap": {},
    "rate_spread": [],
    "us_rate_spread": []
}

def _fetch_task(name, func, *args):
    """ê°œë³„ ì„œë¹„ìŠ¤ í˜¸ì¶œì„ ë˜í•‘í•˜ì—¬ (key, result) íŠœí”Œì„ ë°˜í™˜"""
    try:
        result = func(*args)
        logger.info(f"âœ… [Scheduler] {name} updated")
        return (name, result)
    except Exception as e:
        logger.error(f"âŒ [Scheduler] {name} failed: {e}")
        return (name, None)

def update_all_data():
    """
    Background Task: Fetches data from all services and updates DATA_STORE.
    ThreadPoolExecutorë¡œ ëª¨ë“  ì„œë¹„ìŠ¤ë¥¼ ë³‘ë ¬ ì‹¤í–‰í•˜ì—¬ ì „ì²´ ì—…ë°ì´íŠ¸ ì‹œê°„ì„ ë‹¨ì¶•.
    """
    logger.info(f"ğŸ”„ [Scheduler] Starting data update at {datetime.now(ZoneInfo('Asia/Seoul'))}...")

    tasks = {
        "market_pulse": (stock_service.get_market_pulse,),
        "cpi": (macro_service.get_macro_data, "CPIAUCSL", "US CPI (Consumer Price Index)"),
        "unrate": (macro_service.get_macro_data, "UNRATE", "US Unemployment Rate"),
        "risk_ratio": (analysis_service.get_risk_ratio,),
        "credit_spread": (bond_service.get_credit_spread_data,),
        "yield_gap": (analysis_service.get_yield_gap_data,),
        "rate_spread": (analysis_service.get_rate_spread_data,),
        "us_rate_spread": (analysis_service.get_us_rate_spread_data,),
    }

    with ThreadPoolExecutor(max_workers=len(tasks)) as executor:
        futures = {
            executor.submit(_fetch_task, key, *funcs): key
            for key, funcs in tasks.items()
        }
        for future in as_completed(futures):
            key, result = future.result()
            if result is not None:
                DATA_STORE[key] = result

    logger.info("âœ¨ [Scheduler] All updates completed.")

def start_scheduler():
    """
    Initializes and starts the BackgroundScheduler.
    """
    scheduler = BackgroundScheduler(timezone=ZoneInfo("Asia/Seoul"))
    
    # Add job: Run every 20 minutes
    scheduler.add_job(update_all_data, 'interval', minutes=20, id='update_all')
    
    # Run immediately on startup (in a separate thread to avoid blocking startup)
    # or just let the scheduler pick it up. 
    # To have data immediately, we can run it once safely here or handle "Loading" in frontend.
    # Let's run it once synchronously for simplicity (or Threaded) to populate cache.
    # But for fast startup, purely async is better. Let's just create the scheduler.
    # We will trigger an initial update explicitly in main.py lifespan.
    
    scheduler.start()
    return scheduler
